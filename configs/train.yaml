seed: 42

paths:
  output_dir: "outputs"

trainer:
  _target_: lightning.pytorch.trainer.Trainer
  accelerator: "cpu"
  devices: 1
  min_epochs: 1
  max_epochs: 100
  # precision: null
  accumulate_grad_batches: 1
  log_every_n_steps: 1
  val_check_interval: 1.0
  check_val_every_n_epoch: 1
  fast_dev_run: True

callbacks:
  rich_progress_bar:
    _target_: lightning.pytorch.callbacks.RichProgressBar

  model_summary:
    _target_: lightning.pytorch.callbacks.RichModelSummary
    max_depth: 1

  model_checkpoint:
    _target_: lightning.pytorch.callbacks.ModelCheckpoint
    filename: "epoch_{epoch:03d}"
    dirpath: ${paths.output_dir}/checkpoints
    monitor: "val/loss"
    mode: "min"
    save_top_k: 2
    save_last: True
    auto_insert_metric_name: False
    verbose: True
    save_weights_only: False # if True, then only the modelâ€™s weights will be saved
    every_n_train_steps: null # number of training steps between checkpoints
    train_time_interval: null # checkpoints are monitored at the specified time interval
    every_n_epochs: null # number of epochs between checkpoints
    save_on_train_epoch_end: null # whether to run checkpoint at the end of the training epoch or the end of validation

  early_stopping:
    _target_: lightning.pytorch.callbacks.EarlyStopping
    monitor: "val/loss"
    min_delta: 0.
    mode: "min"
    patience: 10
    verbose: True
    strict: True # whether to crash the training if monitor is not found in the validation metrics
    check_finite: True # when set True, stops training when the monitor becomes NaN or infinite

logger:
  wandb:
    _target_: lightning.pytorch.loggers.wandb.WandbLogger
    project: "gec-torch"
    entity: "andoDsAI"
    save_dir: ${paths.output_dir}
    log_model: False
    prefix: ""
    tags: []
    group: ""
    offline: False

model:
  model_name_or_path: "google/flan-t5-base"
  lightning_module:
    learning_rate: 1e-4
    weight_decay: 0.01
    adam_epsilon: 1e-8
    ignore_index: -100
    is_freeze: True
    freeze_layers: 1
    unfreeze_batch_idx: 1000
    prepend_sentence: "Correct grammar in this sentence: "
    compile: False

tokenizer:
  model_name_or_path: ${model.model_name_or_path}
  max_seq_len: 64
  use_fast: True

generator:
  repetition_penalty: 1.2
  beam_search: True
  num_beams: 5
  early_stopping: True
  max_length: ${tokenizer.max_seq_len}
  no_repeat_ngram_size: 2
  top_k: 2000
  top_p: 0.95

data:
  path: "liweili/c4_200m"
  val_sample: 100000
  train_batch_size: 2
  val_batch_size: 2
  num_workers: 4
  streaming: True
